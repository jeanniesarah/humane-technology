### **Module 6 | Supporting Fairness & Justice**

We’ll discuss how technology can decrease the gap between the privileged and the marginalized and explore ways to prioritize positive social outcomes in design.

_How can technology address inequity and practically integrate voices of people who experience harm?_

---

#### Introduction

In this module, we examine:

- How to recognize the limitations of our own perspective and develop practices that expand it.
- How technology, even built with the best of intentions, often integrates and amplifies social systems that give some people more access to opportunity than others (also known as systemic inequity).
- How humane technologists can support those working to address inequity rather than seeing inequity as a problem for technology to solve.

Many in the technology industry are drawn to technological solutionism, which Evgeny Morozov defines as the _“temptation of the digital age to fix everything—from crime to corruption to pollution to obesity—by digitally quantifying, tracking, or gamifying behavior.”_

Most technologists are trained to craft ambitious, elegant solutions to urgent problems and rapidly scale them to the rest of the world.
Though they often name impact as their primary motivator, these problem-solving innovators are also routinely rewarded with fame and financial success. It’s an enticing proposition, but the resulting competition ultimately leads to a world that prioritizes scale and wealth over careful consideration of consequences.

The problem lies in scale. When a technologist designs a solution for a specific community, particularly one they live in, unintended consequences are unlikely. But when that same solution is scaled, we have to carefully consider three concepts we discussed earlier:

1. The technologies we create reflect what we value, but values differ from community to community. Even if our values inform our designs, another community may not share our values or needs.

2. Communities have different cultures, governments, laws, and economic systems. These differences influence the effects of technology, regardless of an inventor's intentions.

3. Technology subtly affects the human mind. When you massively scale those effects, you will produce unintended and harmful outcomes.

#### Reflection

Do we hear and respond to voices of those who are directly impacted by our product?
Do those impacted by our product have a way to communicate their experience to decision-makers in our organization?
Are there voices that should be better represented in our design process?
Are you uniquely positioned to build trust with groups of people whose perspectives are missing?

#### Key Takeaways

- Humane technologists learn from history to understand and respond to existing societal harms that technology accelerates.

- We must take responsibility for the impact of our products on the lives of others.

- Privilege is being able to not prioritize something that another person must.

- Recognize the ways that your limited experiences and intuition poorly equip you to serve others at a massive scale.

- Systems of inequity shape which human experiences are centered in the design process, which experiences are dismissed as edge cases, and which experiences are not even imagined by design teams.

- Algorithmic bias occurs when machine learning and algorithmic decision-making systems encode and amplify bias and historic inequity.

- Algorithms now set working conditions not just for gig workers, but for content creators and moderators, warehouse workers, and more.

- Aggressively marketing a product in a community that isn’t well understood by your team may require you to invest just as aggressively to mitigate the harms your product could create.

- Build accountable relationships with those underrepresented in the design process.

- Focus first on humbly listening and building trust rather than identifying and solving problems.

​- Recognize and break down barriers to input by directly supporting those who report harm and encouraging those who push for reform.

- Centering the most affected groups in a design process means that community representatives are core to the project's development, with real oversight and a stake in reputational and monetary rewards if it is successful.

#### Resources

To learn more about algorithms and inequality, check out:

- “Weapons of Math Destruction,” a book by Cathy O’Neil

- “Automating Inequality,” a book by Virginia Eubanks

- “Coded Bias,” a film exploring the work of the Algorithmic Justice League (Netflix)

- “Model Cards,” a practical toolkit for evaluating bias in machine learning

​To learn more about bias in the design process:

- Read "Race After Technology," a book by Ruha Benjamin

- Read "Design Justice," a book by Sasha Costanza-Chock

- Review the “Beyond the Statement” framework by Color of Change
